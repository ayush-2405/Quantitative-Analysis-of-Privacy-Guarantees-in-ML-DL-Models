{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a37096f-0dff-4854-a6b4-40564d631851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/OpenMined/PyDP.git\n",
      "  Cloning https://github.com/OpenMined/PyDP.git to /tmp/pip-req-build-qd7qc5kc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/OpenMined/PyDP.git /tmp/pip-req-build-qd7qc5kc\n",
      "\n",
      "  Resolved https://github.com/OpenMined/PyDP.git to commit 01620d5553cc2e6f54b47bc748498f3eb156543c\n",
      "  Running command git submodule update --init --recursive -q\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: python-dp\n",
      "  Building wheel for python-dp (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-dp: filename=python_dp-1.1.5rc4-cp310-cp310-linux_x86_64.whl size=38795 sha256=1ab357d9a1ace9d1f33fafd73a2ae639bcbf19afffcd3f9de91babd0cebefe69\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4one74_h/wheels/c9/b9/e9/cb36261017c51e1d4497438ec8ff121a52d75047f91f7f4c64\n",
      "Successfully built python-dp\n",
      "Installing collected packages: python-dp\n",
      "Successfully installed python-dp-1.1.5rc4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting opacus\n",
      "  Downloading opacus-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.15 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opacus) (1.26.4)\n",
      "Requirement already satisfied: torch>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opacus) (2.2.1+cu121)\n",
      "Requirement already satisfied: scipy>=1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opacus) (1.11.4)\n",
      "Collecting opt-einsum>=3.3.0 (from opacus)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0->opacus) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->opacus) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=2.0->opacus) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n",
      "Downloading opacus-1.5.2-py3-none-any.whl (239 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: opt-einsum, opacus\n",
      "Successfully installed opacus-1.5.2 opt-einsum-3.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir git+https://github.com/OpenMined/PyDP.git\n",
    "!pip install opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d15e0-c848-4a26-9744-15169ae48136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7332659-7b40-4847-9274-a32ac950b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from opacus import PrivacyEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d836af78-94f3-4655-9a7b-551572ee6d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53163058-50a3-436e-992c-4b573d6306c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1) LOAD AND PREPROCESS DATA\n",
    "###############################################################################\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Check for missing values and handle them\n",
    "    # Replace zeros with NaN in certain columns where zero doesn't make sense\n",
    "    cols_to_replace_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "    for col in cols_to_replace_zeros:\n",
    "        data[col] = data[col].replace(0, np.nan)\n",
    "    \n",
    "    # Fill NaN values with the median of each column\n",
    "    for col in cols_to_replace_zeros:\n",
    "        data[col] = data[col].fillna(data[col].median())\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop(columns=['Outcome']).values\n",
    "    y = data['Outcome'].values\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X.astype(np.float32), y.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f09ea4-a797-4eb1-b5f1-6c6742e0bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2) TORCH DATASET\n",
    "###############################################################################\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9c704d-8012-49d5-ac18-16c92a4358ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 3) BUILD A SIMPLE MLP WITH PYTORCH\n",
    "###############################################################################\n",
    "class DeeperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.gn1 = nn.GroupNorm(8, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.gn2 = nn.GroupNorm(8, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.gn3 = nn.GroupNorm(8, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.gn4 = nn.GroupNorm(8, 64)\n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.gn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "210b0df9-742b-4bea-8155-7b38a71eed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4) TRAIN WITH DP-SGD (Opacus) AND EVALUATE\n",
    "###############################################################################\n",
    "def train_dp_sgd(X, y, epochs=10, batch_size=128, lr=0.01, max_grad_norm=1.0, noise_multiplier=1.1, delta=1e-5):\n",
    "    N = len(X)\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    X, y = X[indices], y[indices]\n",
    "    split = int(0.8*N)\n",
    "    X_tr, X_te = X[:split], X[split:]\n",
    "    y_tr, y_te = y[:split], y[split:]\n",
    "    train_ds = MedicalDataset(X_tr, y_tr)\n",
    "    test_ds = MedicalDataset(X_te, y_te)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "    num_classes = len(np.unique(y))\n",
    "    model = DeeperMLP(input_dim, num_classes)\n",
    "    # Switch to SGD with Momentum\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize PrivacyEngine with RDP accountant\n",
    "    privacy_engine = PrivacyEngine(accountant=\"rdp\")\n",
    "    model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * y_batch.size(0)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        scheduler.step(train_loss)\n",
    "        epsilon, best_alpha = privacy_engine.accountant.get_privacy_spent(delta=delta)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss={train_loss:.4f}, Acc={train_acc:.4f}, Eps={epsilon:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_test += (preds == y_batch).sum().item()\n",
    "            total_test += y_batch.size(0)\n",
    "    test_acc = correct_test / total_test\n",
    "    epsilon, best_alpha = privacy_engine.accountant.get_privacy_spent(delta=delta)\n",
    "    print(f\"Final Eps={epsilon:.2f} (delta={delta}), Test Acc={test_acc:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd674a8d-fbb9-46e6-9342-d1e728c71421",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 5) MAIN\n",
    "###############################################################################\n",
    "def main():\n",
    "    csv_path = \"diabetes (3).csv\"\n",
    "    X, y = load_and_preprocess_data(csv_path)\n",
    "    # Adjusted learning rate for SGD\n",
    "    model = train_dp_sgd(X, y, epochs=10, batch_size=64,  # Larger batch size for stability\n",
    "                        lr=0.001,  # Higher learning rate for SGD\n",
    "                        max_grad_norm=1.0,  # Adjusted gradient norm\n",
    "                        noise_multiplier=1.3,  # Adjusted noise multiplier\n",
    "                        delta=1e-5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c11f524-efc5-49e3-9284-bdf142faa61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.7635, Acc: 0.6206, ε: 2.04\n",
      "Epoch 2/10 - Loss: 0.7835, Acc: 0.6074, ε: 2.54\n",
      "Epoch 3/10 - Loss: 0.7432, Acc: 0.6260, ε: 2.95\n",
      "Epoch 4/10 - Loss: 0.6967, Acc: 0.6738, ε: 3.30\n",
      "Epoch 5/10 - Loss: 0.7590, Acc: 0.6398, ε: 3.62\n",
      "Epoch 6/10 - Loss: 0.7560, Acc: 0.6364, ε: 3.92\n",
      "Epoch 7/10 - Loss: 0.7903, Acc: 0.6224, ε: 4.19\n",
      "Epoch 8/10 - Loss: 0.7352, Acc: 0.6588, ε: 4.46\n",
      "Epoch 9/10 - Loss: 0.7369, Acc: 0.6536, ε: 4.70\n",
      "Epoch 10/10 - Loss: 0.7471, Acc: 0.6550, ε: 4.94\n",
      "\n",
      "Final Privacy Budget: ε=4.94, δ=1e-05\n",
      "Test Accuracy: 0.6948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradSampleModule(DeeperMLP(\n",
       "  (fc1): Linear(in_features=8, out_features=512, bias=True)\n",
       "  (gn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (gn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (gn3): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (gn4): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "  (fc5): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b92704-d94c-4d33-8886-4496c7d5e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTGAN Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c28ea39-fa4a-490d-93cd-cdf58eeddcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.6872, Acc: 0.5946, ε: 2.04\n",
      "Epoch 2/10 - Loss: 0.6925, Acc: 0.5608, ε: 2.54\n",
      "Epoch 3/10 - Loss: 0.6986, Acc: 0.5538, ε: 2.95\n",
      "Epoch 4/10 - Loss: 0.6739, Acc: 0.6065, ε: 3.30\n",
      "Epoch 5/10 - Loss: 0.6888, Acc: 0.5969, ε: 3.62\n",
      "Epoch 6/10 - Loss: 0.6595, Acc: 0.6355, ε: 3.92\n",
      "Epoch 7/10 - Loss: 0.6719, Acc: 0.6336, ε: 4.19\n",
      "Epoch 8/10 - Loss: 0.6708, Acc: 0.6327, ε: 4.46\n",
      "Epoch 9/10 - Loss: 0.6585, Acc: 0.6413, ε: 4.70\n",
      "Epoch 10/10 - Loss: 0.7205, Acc: 0.6231, ε: 4.94\n",
      "\n",
      "Final Privacy Budget: ε=4.94, δ=1e-05\n",
      "Test Accuracy: 0.6883\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "###############################################################################\n",
    "# 1) LOAD AND PREPROCESS DATA\n",
    "###############################################################################\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('Outcome', axis=1).values\n",
    "    y = df['Outcome'].values\n",
    "    \n",
    "    # Handle zero values in features where zero is invalid\n",
    "    # Columns: Glucose, BloodPressure, SkinThickness, Insulin, BMI\n",
    "    invalid_zero_cols = [1, 2, 3, 4, 5]  # Indices of the columns to impute\n",
    "    for col in invalid_zero_cols:\n",
    "        col_data = X[:, col]\n",
    "        # Replace zeros with the mean of the column (ignoring zeros)\n",
    "        col_mean = np.mean(col_data[col_data != 0])\n",
    "        col_data[col_data == 0] = col_mean\n",
    "        X[:, col] = col_data\n",
    "    \n",
    "    return X.astype(np.float32), y.astype(np.int64)\n",
    "\n",
    "###############################################################################\n",
    "# 2) TORCH DATASET\n",
    "###############################################################################\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "###############################################################################\n",
    "# 3) BUILD A SIMPLE MLP WITH PYTORCH\n",
    "###############################################################################\n",
    "class DeeperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2):  # Fixed num_classes for binary classification\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.gn1 = nn.GroupNorm(8, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.gn2 = nn.GroupNorm(8, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.gn3 = nn.GroupNorm(8, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.gn4 = nn.GroupNorm(8, 64)\n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.gn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "###############################################################################\n",
    "# 4) TRAIN WITH DP-SGD (Opacus) AND EVALUATE\n",
    "###############################################################################\n",
    "def train_dp_sgd(X, y, epochs=10, batch_size=64, lr=0.001, max_grad_norm=1.0, noise_multiplier=1.3, delta=1e-5):\n",
    "    # Split into training and test sets\n",
    "    N = len(X)\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(0.8 * N)\n",
    "    X_train, X_test = X[indices[:split]], X[indices[split:]]\n",
    "    y_train, y_test = y[indices[:split]], y[indices[split:]]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_ds = MedicalDataset(X_train, y_train)\n",
    "    test_ds = MedicalDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_dim = X.shape[1]\n",
    "    model = DeeperMLP(input_dim)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Attach PrivacyEngine\n",
    "    privacy_engine = PrivacyEngine(accountant=\"rdp\")\n",
    "    model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch.float())\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * y_batch.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        scheduler.step(epoch_loss)\n",
    "        \n",
    "        epsilon = privacy_engine.get_epsilon(delta)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, ε: {epsilon:.2f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch.float())\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_test += (preds == y_batch).sum().item()\n",
    "            total_test += y_batch.size(0)\n",
    "    \n",
    "    test_acc = correct_test / total_test\n",
    "    epsilon = privacy_engine.get_epsilon(delta)\n",
    "    print(f\"\\nFinal Privacy Budget: ε={epsilon:.2f}, δ={delta}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# 5) MAIN\n",
    "###############################################################################\n",
    "def main():\n",
    "    csv_path = \"Synthetic_generated_ACTGAN_Daibetes.csv\"  # Update path as needed\n",
    "    X, y = load_and_preprocess_data(csv_path)\n",
    "    \n",
    "    # Train with DP-SGD\n",
    "    model = train_dp_sgd(\n",
    "        X, y,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        lr=0.001,\n",
    "        max_grad_norm=1.0,\n",
    "        noise_multiplier=1.3,\n",
    "        delta=1e-5\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b2693-b22e-49d3-9040-865f24328d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit Customers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4cd947b-fca9-4f03-ae65-44ccc25ef357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.7908 | Acc: 0.4147 | ε: 6.59\n",
      "Epoch 2/10 | Loss: 0.6948 | Acc: 0.5610 | ε: 8.04\n",
      "Epoch 3/10 | Loss: 0.6172 | Acc: 0.6946 | ε: 9.15\n",
      "Epoch 4/10 | Loss: 0.6365 | Acc: 0.7118 | ε: 10.12\n",
      "Epoch 5/10 | Loss: 0.8027 | Acc: 0.6771 | ε: 11.01\n",
      "Epoch 6/10 | Loss: 0.9114 | Acc: 0.6912 | ε: 11.80\n",
      "Epoch 7/10 | Loss: 1.0091 | Acc: 0.7000 | ε: 12.57\n",
      "Epoch 8/10 | Loss: 1.0552 | Acc: 0.7018 | ε: 13.27\n",
      "Epoch 9/10 | Loss: 1.0148 | Acc: 0.7299 | ε: 13.98\n",
      "Epoch 10/10 | Loss: 1.1767 | Acc: 0.6968 | ε: 14.62\n",
      "\n",
      "Final Privacy: ε=14.62, δ=1e-05 | Test Accuracy: 0.6950\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "###############################################################################\n",
    "# 1) DATA PREPROCESSING\n",
    "###############################################################################\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Define numerical and categorical columns\n",
    "    numerical_cols = ['duration', 'creditamount', 'installmentcommitment', \n",
    "                      'residencesince', 'age', 'existingcredits', 'numdependents']\n",
    "    categorical_cols = ['checkingstatus', 'credithistory', 'purpose', 'savingsstatus', \n",
    "                         'employment', 'personalstatus', 'otherparties', 'propertymagnitude', \n",
    "                         'otherpaymentplans', 'housing', 'job', 'owntelephone', 'foreignworker']\n",
    "    \n",
    "    # Separate target\n",
    "    y = df['class'].values.astype(int)\n",
    "    df = df.drop(columns=['class'])\n",
    "    \n",
    "    # Process numerical features\n",
    "    numerical_data = df[numerical_cols].astype(float)\n",
    "    scaler = StandardScaler()\n",
    "    numerical_data = scaler.fit_transform(numerical_data)\n",
    "    \n",
    "    # Process categorical features (one-hot encoding)\n",
    "    categorical_data = pd.get_dummies(df[categorical_cols], columns=categorical_cols)\n",
    "    \n",
    "    # Combine features\n",
    "    X = np.hstack([numerical_data, categorical_data.values.astype(float)])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "###############################################################################\n",
    "# 2) TORCH DATASET\n",
    "###############################################################################\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).long()  # CrossEntropyLoss expects long integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "###############################################################################\n",
    "# 3) BUILD A SIMPLE MLP WITH PYTORCH\n",
    "###############################################################################\n",
    "class DeeperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2):  # Binary classification\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.gn1 = nn.GroupNorm(8, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.gn2 = nn.GroupNorm(8, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.gn3 = nn.GroupNorm(8, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.gn4 = nn.GroupNorm(8, 64)\n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.gn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "###############################################################################\n",
    "# 4) TRAIN WITH DP-SGD (Opacus) AND EVALUATE\n",
    "###############################################################################\n",
    "def train_dp_sgd(X, y, epochs=10, batch_size=128, lr=0.01, \n",
    "                 max_grad_norm=1.0, noise_multiplier=1.1, delta=1e-5):\n",
    "    # Shuffle and split data\n",
    "    N = len(X)\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    X, y = X[indices], y[indices]\n",
    "    split = int(0.8 * N)\n",
    "    X_tr, X_te = X[:split], X[split:]\n",
    "    y_tr, y_te = y[:split], y[split:]\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_ds = MedicalDataset(X_tr, y_tr)\n",
    "    test_ds = MedicalDataset(X_te, y_te)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    input_dim = X.shape[1]\n",
    "    num_classes = len(np.unique(y))\n",
    "    model = DeeperMLP(input_dim, num_classes)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Add DP constraints\n",
    "    privacy_engine = PrivacyEngine(accountant=\"rdp\")\n",
    "    model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * y_batch.size(0)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        \n",
    "        # Metrics and scheduler\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        scheduler.step(train_loss)\n",
    "        epsilon, _ = privacy_engine.accountant.get_privacy_spent(delta=delta)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | ε: {epsilon:.2f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_test += (preds == y_batch).sum().item()\n",
    "            total_test += y_batch.size(0)\n",
    "    \n",
    "    test_acc = correct_test / total_test\n",
    "    epsilon, _ = privacy_engine.accountant.get_privacy_spent(delta=delta)\n",
    "    print(f\"\\nFinal Privacy: ε={epsilon:.2f}, δ={delta} | Test Accuracy: {test_acc:.4f}\")\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# 5) MAIN\n",
    "###############################################################################\n",
    "def main():\n",
    "    csv_path = \"credit_customers.csv\"\n",
    "    X, y = load_and_preprocess_data(csv_path)\n",
    "    \n",
    "    # Train with DP-SGD\n",
    "    model = train_dp_sgd(\n",
    "        X, y,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        lr=0.005,\n",
    "        max_grad_norm=1.0,\n",
    "        noise_multiplier=0.7,\n",
    "        delta=1e-5\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089564d2-3378-47df-bed3-7a072fbfed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTGAN Credit Customers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d569f5-dda5-4397-9d80-5c0f26aadaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.6599 | Acc: 0.6341 | ε: 6.59\n",
      "Epoch 2/10 | Loss: 0.6710 | Acc: 0.6642 | ε: 8.04\n",
      "Epoch 3/10 | Loss: 0.7742 | Acc: 0.6285 | ε: 9.15\n",
      "Epoch 4/10 | Loss: 0.7979 | Acc: 0.6626 | ε: 10.12\n",
      "Epoch 5/10 | Loss: 0.9566 | Acc: 0.6282 | ε: 11.01\n",
      "Epoch 6/10 | Loss: 0.9606 | Acc: 0.6612 | ε: 11.80\n",
      "Epoch 7/10 | Loss: 0.9567 | Acc: 0.6749 | ε: 12.57\n",
      "Epoch 8/10 | Loss: 1.0830 | Acc: 0.6563 | ε: 13.27\n",
      "Epoch 9/10 | Loss: 1.0575 | Acc: 0.6659 | ε: 13.98\n",
      "Epoch 10/10 | Loss: 1.1835 | Acc: 0.6358 | ε: 14.62\n",
      "\n",
      "Final Privacy: ε=14.62, δ=1e-05 | Test Accuracy: 0.6800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "###############################################################################\n",
    "# 1) DATA PREPROCESSING\n",
    "###############################################################################\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Define numerical and categorical columns\n",
    "    numerical_cols = ['duration', 'creditamount', 'installmentcommitment', \n",
    "                      'residencesince', 'age', 'existingcredits', 'numdependents']\n",
    "    categorical_cols = ['checkingstatus', 'credithistory', 'purpose', 'savingsstatus', \n",
    "                         'employment', 'personalstatus', 'otherparties', 'propertymagnitude', \n",
    "                         'otherpaymentplans', 'housing', 'job', 'owntelephone', 'foreignworker']\n",
    "    \n",
    "    # Separate target\n",
    "    y = df['class'].values.astype(int)\n",
    "    df = df.drop(columns=['class'])\n",
    "    \n",
    "    # Process numerical features\n",
    "    numerical_data = df[numerical_cols].astype(float)\n",
    "    scaler = StandardScaler()\n",
    "    numerical_data = scaler.fit_transform(numerical_data)\n",
    "    \n",
    "    # Process categorical features (one-hot encoding)\n",
    "    categorical_data = pd.get_dummies(df[categorical_cols], columns=categorical_cols)\n",
    "    \n",
    "    # Combine features\n",
    "    X = np.hstack([numerical_data, categorical_data.values.astype(float)])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "###############################################################################\n",
    "# 2) TORCH DATASET\n",
    "###############################################################################\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).long()  # CrossEntropyLoss expects long integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "###############################################################################\n",
    "# 3) BUILD A SIMPLE MLP WITH PYTORCH\n",
    "###############################################################################\n",
    "class DeeperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2):  # Binary classification\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.gn1 = nn.GroupNorm(8, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.gn2 = nn.GroupNorm(8, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.gn3 = nn.GroupNorm(8, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.gn4 = nn.GroupNorm(8, 64)\n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.gn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "###############################################################################\n",
    "# 4) TRAIN WITH DP-SGD (Opacus) AND EVALUATE\n",
    "###############################################################################\n",
    "def train_dp_sgd(X, y, epochs=10, batch_size=128, lr=0.01, \n",
    "                 max_grad_norm=1.0, noise_multiplier=1.1, delta=1e-5):\n",
    "    # Shuffle and split data\n",
    "    N = len(X)\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    X, y = X[indices], y[indices]\n",
    "    split = int(0.8 * N)\n",
    "    X_tr, X_te = X[:split], X[split:]\n",
    "    y_tr, y_te = y[:split], y[split:]\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_ds = MedicalDataset(X_tr, y_tr)\n",
    "    test_ds = MedicalDataset(X_te, y_te)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    input_dim = X.shape[1]\n",
    "    num_classes = len(np.unique(y))\n",
    "    model = DeeperMLP(input_dim, num_classes)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Add DP constraints\n",
    "    privacy_engine = PrivacyEngine(accountant=\"rdp\")\n",
    "    model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * y_batch.size(0)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        \n",
    "        # Metrics and scheduler\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        scheduler.step(train_loss)\n",
    "        epsilon, _ = privacy_engine.accountant.get_privacy_spent(delta=delta)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | ε: {epsilon:.2f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_test += (preds == y_batch).sum().item()\n",
    "            total_test += y_batch.size(0)\n",
    "    \n",
    "    test_acc = correct_test / total_test\n",
    "    epsilon, _ = privacy_engine.accountant.get_privacy_spent(delta=delta)\n",
    "    print(f\"\\nFinal Privacy: ε={epsilon:.2f}, δ={delta} | Test Accuracy: {test_acc:.4f}\")\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# 5) MAIN\n",
    "###############################################################################\n",
    "def main():\n",
    "    csv_path = \"ACTGAN_generated.csv\"\n",
    "    X, y = load_and_preprocess_data(csv_path)\n",
    "    \n",
    "    # Train with DP-SGD\n",
    "    model = train_dp_sgd(\n",
    "        X, y,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        lr=0.005,\n",
    "        max_grad_norm=1.0,\n",
    "        noise_multiplier=0.7,\n",
    "        delta=1e-5\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c82b5-a9a9-4fe3-b50b-2e3ce3597d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Employee Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4462aaa0-b66b-4b3f-b75d-006321c6b9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['Education', 'JoiningYear', 'City', 'PaymentTier', 'Age', 'Gender', 'EverBenched', 'ExperienceInCurrentDomain', 'LeaveOrNot']\n",
      "Sample data:\n",
      "    Education  JoiningYear       City  PaymentTier  Age  Gender EverBenched  \\\n",
      "0  Bachelors         2017  Bangalore            3   34    Male          No   \n",
      "1  Bachelors         2013       Pune            1   28  Female          No   \n",
      "2  Bachelors         2014  New Delhi            3   38  Female          No   \n",
      "3    Masters         2016  Bangalore            3   27    Male          No   \n",
      "4    Masters         2017       Pune            3   24    Male         Yes   \n",
      "\n",
      "   ExperienceInCurrentDomain  LeaveOrNot  \n",
      "0                          0           0  \n",
      "1                          3           1  \n",
      "2                          2           0  \n",
      "3                          5           1  \n",
      "4                          2           1  \n",
      "Processed data shape: (4653, 10)\n",
      "Target shape: (4653,)\n",
      "Number of classes: 2\n",
      "Epoch 1/10, Loss=0.9354, Acc=0.3732, Eps=0.79\n",
      "Epoch 2/10, Loss=0.8098, Acc=0.4221, Eps=0.92\n",
      "Epoch 3/10, Loss=0.6987, Acc=0.5580, Eps=1.04\n",
      "Epoch 4/10, Loss=0.6565, Acc=0.6497, Eps=1.15\n",
      "Epoch 5/10, Loss=0.7116, Acc=0.6660, Eps=1.25\n",
      "Epoch 6/10, Loss=0.8312, Acc=0.6469, Eps=1.35\n",
      "Epoch 7/10, Loss=0.9361, Acc=0.6616, Eps=1.45\n",
      "Epoch 8/10, Loss=1.0335, Acc=0.6651, Eps=1.54\n",
      "Epoch 9/10, Loss=1.0770, Acc=0.6604, Eps=1.63\n",
      "Epoch 10/10, Loss=1.1142, Acc=0.6574, Eps=1.72\n",
      "Final Eps=1.72 (delta=1e-05), Test Acc=0.6498\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from opacus import PrivacyEngine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "###############################################################################\n",
    "# 1) LOAD AND PREPROCESS DATA\n",
    "###############################################################################\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Check the columns\n",
    "    print(\"Columns in the dataset:\", df.columns.tolist())\n",
    "    print(\"Sample data:\\n\", df.head())\n",
    "    \n",
    "    # Extract features and target\n",
    "    X = df.drop('LeaveOrNot', axis=1)\n",
    "    y = df['LeaveOrNot']\n",
    "    \n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = ['Education', 'City', 'Gender', 'EverBenched']\n",
    "    numerical_cols = ['JoiningYear', 'PaymentTier', 'Age', 'ExperienceInCurrentDomain']\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
    "        ])\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_processed = X_processed.toarray() if hasattr(X_processed, 'toarray') else X_processed\n",
    "    y_np = y.values\n",
    "    \n",
    "    print(f\"Processed data shape: {X_processed.shape}\")\n",
    "    print(f\"Target shape: {y_np.shape}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y_np))}\")\n",
    "    \n",
    "    return X_processed.astype(np.float32), y_np.astype(np.int64)\n",
    "\n",
    "###############################################################################\n",
    "# 2) TORCH DATASET\n",
    "###############################################################################\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "###############################################################################\n",
    "# 3) BUILD A SIMPLE MLP WITH PYTORCH\n",
    "###############################################################################\n",
    "class DeeperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.gn1 = nn.GroupNorm(8, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.gn2 = nn.GroupNorm(8, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.gn3 = nn.GroupNorm(8, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.gn4 = nn.GroupNorm(8, 64)\n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.gn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "###############################################################################\n",
    "# 4) TRAIN WITH DP-SGD (Opacus) AND EVALUATE\n",
    "###############################################################################\n",
    "def train_dp_sgd(X, y, epochs=10, batch_size=128, lr=0.01, max_grad_norm=1.0, noise_multiplier=1.1, delta=1e-5):\n",
    "    N = len(X)\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    X, y = X[indices], y[indices]\n",
    "    split = int(0.8*N)\n",
    "    X_tr, X_te = X[:split], X[split:]\n",
    "    y_tr, y_te = y[:split], y[split:]\n",
    "    train_ds = MedicalDataset(X_tr, y_tr)\n",
    "    test_ds = MedicalDataset(X_te, y_te)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "    num_classes = len(np.unique(y))\n",
    "    model = DeeperMLP(input_dim, num_classes)\n",
    "    # Switch to SGD with Momentum\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize PrivacyEngine with RDP accountant\n",
    "    privacy_engine = PrivacyEngine(accountant=\"rdp\")\n",
    "    model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * y_batch.size(0)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        scheduler.step(train_loss)\n",
    "        epsilon, best_alpha = privacy_engine.accountant.get_privacy_spent(delta=delta)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss={train_loss:.4f}, Acc={train_acc:.4f}, Eps={epsilon:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_test += (preds == y_batch).sum().item()\n",
    "            total_test += y_batch.size(0)\n",
    "        test_acc = correct_test / total_test\n",
    "        epsilon, best_alpha = privacy_engine.accountant.get_privacy_spent(delta=delta)\n",
    "        print(f\"Final Eps={epsilon:.2f} (delta={delta}), Test Acc={test_acc:.4f}\")\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# 5) MAIN\n",
    "###############################################################################\n",
    "def main():\n",
    "    csv_path = \"Employee.csv\"\n",
    "    X, y = load_and_preprocess_data(csv_path)\n",
    "    # Adjusted learning rate for SGD\n",
    "    model = train_dp_sgd(X, y, epochs=10, batch_size=64, # Larger batch size for stability\n",
    "                         lr=0.001, # Higher learning rate for SGD\n",
    "                         max_grad_norm=1.0, # Adjusted gradient norm\n",
    "                         noise_multiplier=1.3, # Adjusted noise multiplier\n",
    "                         delta=1e-5)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"dp_sgd_employee_model.pth\")\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec74be-8b53-45e8-a7d8-01e7879f8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTGAN Employee Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bcae70b-c65e-4f29-8e82-f92da528443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['Education', 'JoiningYear', 'City', 'PaymentTier', 'Age', 'Gender', 'EverBenched', 'ExperienceInCurrentDomain', 'LeaveOrNot']\n",
      "Sample data:\n",
      "   Education  JoiningYear       City  PaymentTier  Age  Gender EverBenched  \\\n",
      "0   Masters         2017  New Delhi            2   38    Male          No   \n",
      "1   Masters         2012  Bangalore            3   26    Male          No   \n",
      "2   Masters         2015  New Delhi            2   26  Female          No   \n",
      "3   Masters         2013  New Delhi            3   37  Female          No   \n",
      "4   Masters         2017  New Delhi            2   36    Male          No   \n",
      "\n",
      "   ExperienceInCurrentDomain  LeaveOrNot  \n",
      "0                          2           0  \n",
      "1                          4           0  \n",
      "2                          4           1  \n",
      "3                          1           1  \n",
      "4                          2           1  \n",
      "Processed data shape: (4653, 10)\n",
      "Target shape: (4653,)\n",
      "Number of classes: 2\n",
      "Epoch 1/10, Loss=0.7750, Acc=0.5091, Eps=0.79\n",
      "Epoch 2/10, Loss=0.7763, Acc=0.5103, Eps=0.92\n",
      "Epoch 3/10, Loss=0.7936, Acc=0.5078, Eps=1.04\n",
      "Epoch 4/10, Loss=0.7864, Acc=0.5245, Eps=1.15\n",
      "Epoch 5/10, Loss=0.8109, Acc=0.5143, Eps=1.25\n",
      "Epoch 6/10, Loss=0.7981, Acc=0.5181, Eps=1.35\n",
      "Epoch 7/10, Loss=0.8097, Acc=0.5087, Eps=1.45\n",
      "Epoch 8/10, Loss=0.8025, Acc=0.5181, Eps=1.54\n",
      "Epoch 9/10, Loss=0.7842, Acc=0.5340, Eps=1.63\n",
      "Epoch 10/10, Loss=0.8042, Acc=0.5256, Eps=1.72\n",
      "Final Eps=1.72 (delta=1e-05), Test Acc=0.5521\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from opacus import PrivacyEngine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "###############################################################################\n",
    "# 1) LOAD AND PREPROCESS DATA\n",
    "###############################################################################\n",
    "def load_and_preprocess_data(csv_path):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Check the columns\n",
    "    print(\"Columns in the dataset:\", df.columns.tolist())\n",
    "    print(\"Sample data:\\n\", df.head())\n",
    "    \n",
    "    # Extract features and target\n",
    "    X = df.drop('LeaveOrNot', axis=1)\n",
    "    y = df['LeaveOrNot']\n",
    "    \n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = ['Education', 'City', 'Gender', 'EverBenched']\n",
    "    numerical_cols = ['JoiningYear', 'PaymentTier', 'Age', 'ExperienceInCurrentDomain']\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
    "        ])\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_processed = X_processed.toarray() if hasattr(X_processed, 'toarray') else X_processed\n",
    "    y_np = y.values\n",
    "    \n",
    "    print(f\"Processed data shape: {X_processed.shape}\")\n",
    "    print(f\"Target shape: {y_np.shape}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y_np))}\")\n",
    "    \n",
    "    return X_processed.astype(np.float32), y_np.astype(np.int64)\n",
    "\n",
    "###############################################################################\n",
    "# 2) TORCH DATASET\n",
    "###############################################################################\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "###############################################################################\n",
    "# 3) BUILD A SIMPLE MLP WITH PYTORCH\n",
    "###############################################################################\n",
    "class DeeperMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.gn1 = nn.GroupNorm(8, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.gn2 = nn.GroupNorm(8, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.gn3 = nn.GroupNorm(8, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.gn4 = nn.GroupNorm(8, 64)\n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.gn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.gn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "###############################################################################\n",
    "# 4) TRAIN WITH DP-SGD (Opacus) AND EVALUATE\n",
    "###############################################################################\n",
    "def train_dp_sgd(X, y, epochs=10, batch_size=128, lr=0.01, max_grad_norm=1.0, noise_multiplier=1.1, delta=1e-5):\n",
    "    N = len(X)\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    X, y = X[indices], y[indices]\n",
    "    split = int(0.8*N)\n",
    "    X_tr, X_te = X[:split], X[split:]\n",
    "    y_tr, y_te = y[:split], y[split:]\n",
    "    train_ds = MedicalDataset(X_tr, y_tr)\n",
    "    test_ds = MedicalDataset(X_te, y_te)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "    num_classes = len(np.unique(y))\n",
    "    model = DeeperMLP(input_dim, num_classes)\n",
    "    # Switch to SGD with Momentum\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize PrivacyEngine with RDP accountant\n",
    "    privacy_engine = PrivacyEngine(accountant=\"rdp\")\n",
    "    model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * y_batch.size(0)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        scheduler.step(train_loss)\n",
    "        epsilon, best_alpha = privacy_engine.accountant.get_privacy_spent(delta=delta)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss={train_loss:.4f}, Acc={train_acc:.4f}, Eps={epsilon:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_test += (preds == y_batch).sum().item()\n",
    "            total_test += y_batch.size(0)\n",
    "        test_acc = correct_test / total_test\n",
    "        epsilon, best_alpha = privacy_engine.accountant.get_privacy_spent(delta=delta)\n",
    "        print(f\"Final Eps={epsilon:.2f} (delta={delta}), Test Acc={test_acc:.4f}\")\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# 5) MAIN\n",
    "###############################################################################\n",
    "def main():\n",
    "    csv_path = \"tabular-actgan-employee.csv\"\n",
    "    X, y = load_and_preprocess_data(csv_path)\n",
    "    # Adjusted learning rate for SGD\n",
    "    model = train_dp_sgd(X, y, epochs=10, batch_size=64, # Larger batch size for stability\n",
    "                         lr=0.001, # Higher learning rate for SGD\n",
    "                         max_grad_norm=1.0, # Adjusted gradient norm\n",
    "                         noise_multiplier=1.3, # Adjusted noise multiplier\n",
    "                         delta=1e-5)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"dp_sgd_employee_model.pth\")\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644f123-5ea1-4b4e-81fb-48c41edcca65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
